---
title: "Statistics with R"
output: 
  html_document:
    fig_height: 4
    fig_width: 8
    highlight: haddock
    theme: united
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE,message=FALSE}
#source("R/common.R")
```

# Preamble

### Needed libraries for this tutorial:
```{r, message = FALSE, warning = FALSE, results='hide'}
library(tidyverse)
library(lsmeans)
library(broom)
library(lubridate)
library(modelr)
```

### Getting example data into R

```{r,message=FALSE,warning=FALSE}
minke <- read_csv("data/minke.csv")
load("data/nsibts_tidy.rda")
```

and do a bit of data processing:

```{r}
d <- 
  tidy_ag %>% 
  filter(latin == 'Gadus morhua', !is.na(wgt),!is.na(age)) %>% 
  left_join(tidy_st) %>% 
  filter(!is.na(depth))
```


# Today

We will review some of the statistical methods available in the R environment and present pointers on how to use and present the results. Among the topics covered are

* Linear regression
* Non-linear models
* Inferences related to categorical variables
* Inferences related to numerical variables
* Analysis of variance

Here it is assumed that the course participants are familiar with basic concepts of statistics, but for those who are interested there are a number of good resources on-line, among those are:

* [OpenIntroStatistics](https://www.openintro.org/stat/)
* [Practical Regression and Anova using R](http://cran.hafro.is/doc/contrib/Faraway-PRA.pdf)
* [Tutor-Web](http://tutor-web.net)
* [Stats on StackExchange](http://stats.stackexchange.com/)



# Linear regression

When fitting statistical models the aim is to "tweak" model settings until they can "mimic" the data optimally. In the case of linear regression we want to estimate the best line through a cloud of points:
```{r,echo=FALSE}
dat <- data_frame(x=1:100/10,y=x + rnorm(100,sd=1)) 
dat %>% 
  ggplot(aes(x,y)) + geom_point()
```


The best line is the one that minimizes the difference between the data and the (model) predictions. Typically the model is of the form:
$$ y = \alpha + \beta x + \ldots $$
we want to choose parameters $\alpha$, $\beta$ and others such that this distance is minimized:

```{r,echo=FALSE}
dat %>% mutate(yhat = 2*x-5) %>% ggplot(aes(x,y)) + geom_point() + geom_line(aes(y=yhat)) + geom_segment(aes(xend=x,yend=yhat),col='blue') + annotate(x=1,y=10,label='a=-5,b=2',geom='label')
```
We can of course try a number of values for $\alpha$ and $\beta$ but essentially we want to minimize the total lengths of the "blue bars" in the figure above. Mathematially speaking this is done by minimizing the following sum:

$$ l = \sum_i \big(y_i - (\alpha + \beta x + \ldots)\big)^2$$
and by varying the $\alpha$-s and $\beta$-s systematically we can dervive the optimum solution. 

```{r,echo=FALSE,message=FALSE,warning=FALSE}
library(viridis)
sim1_dist <- function(a,b){
  sum((dat$y-(a+b*dat$x))^2)
}

grid <- expand.grid(
  a1 = seq(-10, 10, length = 25),
  a2 = seq(-1, 2, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist)) + 
  scale_color_viridis() +
  labs(x=expression(alpha),y=expression(beta))

```

The figure above illustrates the value of $l$, i.e. distance metric, as a function of $\alpha$ and $\beta$. From the figure we see that the minimum is obtained close to $\alpha = 0$ and $\beta = 1$ and that the estimates of $\alpha$ and $\beta$ are negatively correlated. 

In addition to finding the optimal values of $\alpha$ and $\beta$ we want to know if these values are significantly different from zero. In mathematical terms we want to draw  inferences of the form:
$$H_0 : \qquad \beta = 0 $$
vs.
$$H_1 : \qquad \beta \neq 0 $$

### Linear models in R

R provides a suite of tools for model building. One of the simplest types of models, linear regression models, are created using the `lm` function:
```{r}
fit <- lm(y~x, data=dat)
fit
```
where `y~x` is the formula to which parameter should be estimated, `y` is the dependent variable and `x` is explanatory variable and the `~` indicates that the variable `x` should be used to explain `y`. Formulas calls on columns from the data and can be composed of number of components:

```{r,eval=FALSE}
+       ## add variables to the formula
-       ## remove variable
:       ## interaction, written x:y
*       ## x*y is a shorthand for x + y + x:y
/       ## x/y is a shorthand for x + x:y
|       ## x|y is x conditioned on the values of y
I()     ## include a new variable based on the calculations within the parenthesis
-1      ## no intercept term
```


The `lm` function the estimates the coefficients of the formula, and note that `lm` assumes that an intercept ($\alpha$) is estimated as well as the slope ($\beta$). To investigate if the values are significantly different from zero we can use the `summary` function:
```{r}
summary(fit)
```

The output from the `summary` function shows the key statistics from a model. The model coefficients are tabulated along with standard error estimates and p-values. To extract these numbers expliticly as a data frame the `broom` package provides a nice function called `tidy`:
```{r}
tidy(fit)
```
and the confidence intervals can be calculated as:
```{r}
tidy(fit,conf.int = TRUE)
```



And to plot the prediction and residuals the `modelr` package has two functions, `add_predictions` and `add_residuals`:
```{r,out.width = "50%",fig.show = "hold",warning=FALSE,message=FALSE}
aug.dat <- 
  dat %>% 
  add_predictions(fit) %>% 
  add_residuals(fit) 
## plot predictions
aug.dat %>%   
  ggplot(aes(x,y)) + geom_point() + geom_line(aes(y=pred),col='blue')
## plot residuals
aug.dat %>% 
  ggplot(aes(resid)) + geom_histogram()
```



### Fitting length--weight
Now as a real example consider the length--weight relationship for cod:
```{r}
ggplot(d,aes(length,wgt,col=sex)) + geom_point()
```

we see that the relationship between length and weight is probably not linear but as first approximation try that:
```{r}
fit <- lm(wgt~length,data=d)
fit
```
hmm fish a 0 cm is -2.4 kg:) That is not really plausible. But look at the usual summaries:
```{r}
summary(fit)
```
Everything is highly significant, as you would expect with this wealth of data, but looking at the fit to the data and residual:

```{r, echo=FALSE,out.width = "50%",fig.show = "hold",warning=FALSE,message=FALSE}
aug.dat <- 
  d %>% 
  add_predictions(fit) %>% 
  add_residuals(fit) 
## plot predictions
aug.dat %>%   
  ggplot(aes(length,wgt)) + geom_point() + geom_line(aes(y=pred),col='blue')
## plot residuals
aug.dat %>% 
  ggplot(aes(resid)) + geom_histogram()
```

we see immediately that the model does not perform well on the tails and the residual error is heavily skewed. So let's do something more realistic and log transform the data:
```{r}
fit <- lm(log(wgt)~log(length),data=d)
fit
```

This looks more sensible. Now lets plot the results:
```{r}
aug.dat <- 
  d %>% 
  add_predictions(fit) %>% 
  add_residuals(fit) 
## plot predictions, note we need transform predictions back to non-log space
aug.dat %>%   
  ggplot(aes(length,wgt)) + geom_point() + geom_line(aes(y=exp(pred)),col='blue')
## plot residuals
aug.dat %>% 
  ggplot(aes(resid)) + geom_histogram()
```

We can then add variable to the regression by adding to the formula:
```{r}
fit2 <- lm(log(wgt)~log(length)+sex,data=d)
summary(fit2)
```

and we can also add interaction terms, where the `log10(length)*sex` is a shorthand notation for `log10(length) + sex + log10(length):sex`:
```{r}
fit3 <- lm(log(wgt)~log(length)*sex,data=d)
summary(fit3)
```




<div class="panel panel-warning">
<div class="panel-heading">Exercise</div>
<div class="panel-body">

Using the model for length and weight for cod

* Try adding more variables to see if they are signifcant


</div>
</div>



## Model fitting and diagnostics

There is a whole host of different tools that are useful when building regression type models. Typically when selecting what variables should be in the model some sort of model selection criterion is used. Commonly the Akaike Information Criterion (AIC) is used. The AIC is formulated as:

$$AIC = 2k - 2ln(l) $$
where $k$ is the number of parameters in the model and the $l$ is likelihood value, i.e. the distance metric. This essentially means that AIC punishes for every additional parameter so the improvement associated with the extra parameter must be that much greater. R has a built in function `AIC` that calulates the AIC which can be computed iteratively during the selection process. Now taking the most basic model (i.e. with only length) we can compute the AIC as:

```{r}
AIC(fit)
```
and compare with a larger model with the interaction term:
```{r}
AIC(fit3)
```
we see that the AIC score for the larger model is substantially lower, leading to the conclusion that the model with interaction is a better fit. 

We often do this repeatedly starting from a large model and prune the number of variables in the model. To aid in this process R has a function called `drop1` that iteratively drops one model variable out of the fitting process and calculates the AIC score. Starting with a bigger model:

```{r}
fit.full <- lm(log10(wgt)~log10(length)*sex + country + depth + 
                 as.factor(year) + haulno + ship + age + hauldur,
                d)
```

Now do `drop1`:
```{r}
drop1(fit.full)
```
We see that models excluding either `ship` and `hauldur` have a higher AIC score and should therefore be remove, but only one at time. This is of course a bit of manual labour so R provides a function when we want perform a stepwise model selection, i.e the `step` function 
```{r}
fit.reduced <- step(fit.full)
fit.reduced
```

`step` iteratively applies `drop1` to the model until the AIC cannot be improved. 

Next we can plot some diagnostics:
```{r}
plot(fit.reduced)
```

# Generalised linear models
In addition to linear model R has support for generalised linear model (glm) and generalised additive model (gam). These essentially behave similarly as its linear model counter part. The generalised linear model can build a linear model where the response variable is non-gaussian, such as counts data and proportions. To illustrate how this let's build a model for maturity as a function of length. To fit this model we need to separate the immmature fish from the mature fish in our data set.
```{r}
d.mat <- 
  d %>% 
  filter(!is.na(maturity)) %>% 
  ## immature = 0, else mature
  mutate(m = ifelse(maturity %in% c(1,51,61),0,1)) 
```

and now we want to regress the `m` variable to length. Notice that `m` only takes the value 0 or 1 so a regular linear model does not fit. You will need to use a different type of model, a logistic regression, i.e. a model that can handle binomial data. In this is done using the `glm` function with a logistic link function:
```{r}
fit <- glm(m ~ length, data = d.mat, family = binomial)
summary(fit)
```

To plot the data:

```{r}
d.mat %>% 
  add_predictions(fit) %>% 
  ## need to transform the predictions into probabilities
  mutate(pred = plogis(pred)) %>% 
  ggplot(aes(length,pred)) + geom_line()
```

Now the formula for the $l_{50}$ is $-\alpha/\beta$ which we can calulate using the `tidy` function:
```{r}
fit %>% 
  tidy() %>% 
  summarise(l50 = -estimate[1]/estimate[2])
```

You can the continue to build the model exactly as with the `lm` function. Other types of data can be handled with different regression link functions, see the help for the `family` function.


<div class="panel panel-warning">
<div class="panel-heading">Exercise</div>
<div class="panel-body">

Build a model for maturity at age for cod and calculate the $a_50$

</div>
</div>

# Non-linear models
Now say we want to fit a growth curve to our minke whale data. Typically this would by a Von Bertalanffy growth curve of the form:
$$ l = L_{\infty}(1-e^{-k(a-t_0)})$$
How do we do this in R? 

```{r,echo=FALSE,message=FALSE,warning=FALSE}
ggplot(minke,aes(age,length)) + geom_point() + theme_bw() + ylab('Length') + xlab('Age')
```

What do we want to do exactly? Again we want to find the best fitting curve through the datapoints, although now we want estimate a more arbitrary function. This means that we want to "draw" a line that minimized on average the distance to all data points, i.e. find x that solves
$$min_{x} \left(\sum_{i} (l_i - VonB(\textbf{x},a(i)))^2\right)$$

In the Von B function there are three parameters, $L_\infty$, $k$ and $t_0$ that can be adjusted so the task here is to find values of these three parameters such that the above sum is minimized


```{r}
age.data <- filter(minke,!is.na(age))
minke.vonB.par <- 
  nls(length~Linf*(1-exp(-K*(age-t0))),
      data=age.data, start=list(Linf=1100, K=0.1, t0=-1))
minke.vonB.par

```


Formulas in R typically look for variables in the data, in this case the minke whale dataset. If a variable is not in the data, such as variables "Linf", "K" and "t0", they are assumed to be parameters that need to be estimated. Starting values are given in the input as "start". If not given the function may converge to a wrong minima or not at all.

### Confidence intervals
Recall that a 95\% confidence interval represents the potential range of the data, i.e. one can not reject the hypothesis that the parameter estimate is within the range. Confidence intervals can be computed using the following command:
```{r}
minke.vonB.par %>% 
  tidy(conf.int = TRUE)
```

Now lets plot the output:

```{r,warning=FALSE,message=FALSE}
minke %>% 
  add_predictions(minke.vonB.par) %>% 
  ggplot(aes(age,length)) + geom_point() + geom_line(aes(y=pred))
```


<div class="panel panel-warning">
<div class="panel-heading">Exercise</div>
<div class="panel-body">

Using the cod dataset estimate a von Bertalanfy growth curve

</div>
</div>


# Inferences related to categorical variables

## Equality of proportions in a population

Say we have a group $G$, and the probability of being in this group is $p$ (and conversely not being in group $G$ has a probability of $q=1-p$) and we want to investigate the properties of $p$. Then we want to do a binomial test. Essentially this is a hypothesis test of the form
$$H_0:\qquad p = \mu$$ vs one the following $$H_1 : \qquad pÂ¸\neq \mu \quad\vee \quad p < \mu \quad \vee \quad p>\mu $$
In R this is done using `binom.test`:
```{r,eval=FALSE}
binom.test(x = Num.success,   ## number of successes (entries in group G)
           n = Num.trials,    ## total number of trials (size of population)
           p = mu,            ## the hypothesised propability
           alternative = 'two.sided')
```


To illustrate this take the sex ratio from the Ellidaar data and ask the question, is the ratio even? 

```{r}
## restrict the data to only male and females
d2 <- 
  d %>% 
  filter(sex %in% c('M','F'))
```

Now test the hypothesis that the sex ratio is even:

```{r}
tab <- table(d2$sex)

binom.test(tab)
```

We see immediately that the sex ratio is not even in the dataset, i.e. the null hypothesis is rejected. In addition we see that the `binom.test` function calculates an estimate for the ratio and a 95% confidence interval. These values can be saved into a data.frame using the `tidy` function from the `broom` package:

```{r}
binom.test(tab) %>% 
  tidy()
```


## Comparing two or more population proportions

Expanding the binomial test above, say if we wanted to look at whether the sex ratio differs by year, wwe will need to use a $\chi^2$-test. Essentially the $\chi^2$-test compares the propability of being in group $G$ by category $i$, i.e. tests a hypothesis of the form:

$$H_0 : \qquad p_1=p_2=\ldots=p_n$$

vs. 

$$H_1 : \qquad \exists i,j\quad s.t\quad p_i\neq p_j$$

The `chisq.test` has a range of options:
```{r,eval=FALSE}
chisq.test(Cat1,                   ## Factor indicating group assignment
           Cat2,                   ## -- || --
           p,                      ## if y is null, these are the proportions in 
                                   ## each of factor levels
           simulate.p.value = FALSE) ## should the p-value be simulated using MC methods
```

Taking this to our cod data, we want to test if the sex ratio is different between years:
```{r}
chisq.test(d2$sex,d2$year)
```

and again we can use `tidy` to get the estimates and test statistics from the test:

```{r}
chisq.test(d2$sex,d2$year) %>% 
  tidy()
```


<div class="panel panel-warning">
<div class="panel-heading">Exercise</div>
<div class="panel-body">


* Is the proportion of immature fish significantly different between the sexes? 
* Are the number of caught by different vessels significantly different between the sexes?


</div>
</div>




# Inferences related to numerical variables

## Comparing means

When comparing means there are a number of situations that one might be interested in:

* Is the population mean equal to a certain number? 
* Are means of two populations equal?
* Is there an increase between measurements from the same subject?

This is all done using a $t$-test:
```{r,eval=FALSE}
t.test(x,                         ## measurements from population 1
       y,                         ## measurements from population 2
       mu,                        ## true difference between populations
       paired = FALSE,            ## is this a paired test
       alternative = "two.sided") ## the alternative hypothesis
```

Note that by default `t.test` corrects for difference is variance between populations, this can be turned off by setting `var.equal` to `TRUE` when calling `t.test`.

### One population

Looking at the mean length in the river data:

```{r}
t.test(d$length)
```

we see that the mean length is significantly different from 0. Notice that the `t.test` gives the confidence interval for the mean length. 

And as usual we can get the output using `tidy`
```{r}
t.test(d$length) %>% 
  tidy()
```

### Two populations

Lets test if the mean length is significantly different between males and females:
```{r}
males <- d %>% filter(sex == 'M')
females <- d %>% filter(sex == 'F')

t.test(males$length,females$length)
```

and we can set the alternative hypothesis such that the males are smaller than females:
```{r}
t.test(males$length,females$length,alternative = 'less')
```

Then we cannot reject the hypothesis that the true difference is 0. 

## Comparing variance of two (or more) populations

When we want to test wheather the variances of two (or more) populations are equal, i.e. test a hypothesis of the form:

$$H_0 :\qquad \sigma_1^2=\sigma_2^2=\ldots=\sigma_n^2$$
vs.
$$ H_1 : \qquad \exists i,j\quad s.t.\quad \sigma_i^2\neq\sigma_j^2 $$
one can use Bartlett's test of homogeneitiy of variances:
```{r,eval=FALSE}
bartlett.test(x,  ## measurements by group
              g)  ## factor indicating group
```

As an example we can ask if the variances in length are constant between the sexes:
```{r}
bartlett.test(d2$length,
              d2$sex)
```

<div class="panel panel-warning">
<div class="panel-heading">Exercise</div>
<div class="panel-body">


* Do males and females differ significantly in weight?
* What is the confidence interval for the mean length of males?


</div>
</div>



# Analysis of variance

Analysis of variance (or ANOVA) is essentially a test for the equality of means between two (or more) gropus. In general the hypothesis we want to test are of the form:
$$H_0 :\qquad \mu_1=\mu_2=\ldots=\mu_n$$
vs.
$$ H_1 : \qquad \exists i,j\quad s.t.\quad \mu_i\neq\mu_j $$
In R one uses the `aov` function to test this class of hypotheses:
```{r,eval=FALSE}
aov(y~x, data = dat)
```
This need a bit of explaining. The first part of the `aov` input is a formula object. A formula is generally of the form:
$$ y \tilde \quad x$$
where $y$ is the reponse variable and $x$ is the predictor variable. Formulas call on columns from the data and can be composed of number of components:

```{r,eval=FALSE}
+       ## add variables to the formula
-       ## remove variable
:       ## interaction, written x:y
*       ## x*y is a shorthand for x + y + x:y
/       ## x/y is a shorthand for x + x:y
|       ## x|y is x conditioned on the values of y
I()     ## include a new variable based on the calculations within the parenthesis
-1      ## no intercept term
```


## One-way anova
As illustration of how one would perform an ANOVA in R consider the mean length caught by year:

```{r}
ggplot(d,aes(year,length,group=round(year))) + geom_boxplot()
```



```{r}
## note we need change the year to factor 
fit <- aov(length~as.factor(year),data=d)
fit
```
To get the results from the ANOVA one typically needs to use the `summary` function:
```{r}
summary(fit)
```
where we see the mean length is significantly different by year. And as above we can use `tidy`
```{r}
fit %>% tidy()
```

Now this is all well and good, but we now need to know which of these years are significantly different. This can be done using Tukey test, implemented using the `lsmeans` function:
```{r}
fit %>% lsmeans(pairwise~year)
```
But getting the results to a data.frame is bit more involved:
```{r}
ls.fit <- fit %>% lsmeans(pairwise~year)
ls.cont <- ls.fit$contrasts %>% summary() %>% as_data_frame()
ls.cont
```
And to find the years:
```{r}
ls.cont %>% filter(p.value < 0.05)
```

## Two way anova example
As an example of how to do a two way anova with interactions, consider length by sex and year: 
```{r}
d <- d %>% 
  filter(sex %in% c('M','F'))
ggplot(d,aes(year,length, group=interaction(round(year),sex),fill = sex)) +
  geom_boxplot()
```

```{r}
two.way <- aov(length~sex*as.factor(year),data=d)
```
and we can test significance using `summary`:
```{r}
summary(two.way)
```
or using variable deletion using `drop1`:
```{r}
drop1(two.way,test='F')
```
here we see that the interaction is significant. 



<div class="panel panel-warning">
<div class="panel-heading">Exercise</div>
<div class="panel-body">


* Test if the weight is significantly different by year


</div>
</div>

# Other methods

There is a plethora of other modeling methods implemented in the different R-packages, such as random effects models, random forest, gam models, case-crossover models, latent variable models etc.. 



