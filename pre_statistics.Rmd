---
title: "Statistics with R"
output: 
  html_document:
    fig_height: 4
    fig_width: 8
    highlight: haddock
    theme: united
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE,message=FALSE}
#source("R/common.R")
```

# Preamble

### Needed libraries for this tutorial:
```{r, message = FALSE, warning = FALSE, results='hide'}
library(tidyverse)
library(lsmeans)
library(broom)
library(lubridate)
```

### Getting example data into R

```{r,message=FALSE,warning=FALSE}
minke <- read_csv("http://www.hafro.is/~einarhj/data/minke.csv")
load(url("http://www.hafro.is/~einarhj/data/nsibts_tidy.rda"))
```

and do a bit of data processing:

```{r}
d <- 
  ag %>% 
  filter(latin == 'Gadus morhua', !is.na(wgt)) %>% 
  left_join(st)
```


# Today

We will review some of the statistical methods available in the R environment and present pointers on how to use and present the results. Among the topics covered are

* Linear regression
* Non-linear models
* Inferences related to categorical variables
* Inferences related to numerical variables
* Analysis of variance

Here it is assumed that the course participants are familiar with basic concepts of statistics, but for those who are interested there are a number of good resources on-line, among those are:

* [OpenIntroStatistics](https://www.openintro.org/stat/)
* [Practical Regression and Anova using R](http://cran.hafro.is/doc/contrib/Faraway-PRA.pdf)
* [Tutor-Web](http://tutor-web.net)
* [Stats on StackExchange](http://stats.stackexchange.com/)



# Linear regression

In linear regression we want to estimate the best line through a cloud of points:
```{r,echo=FALSE}
data_frame(x=1:100/10,y=x + rnorm(100,sd=1)) %>% 
  ggplot(aes(x,y)) + geom_point()
```


The best line is the one that minimizes the difference between the data and the (model) predictions. Typically the model is of the form:
$$ y = \alpha + \beta x + \ldots $$

And typically the inferences we are interested in are of the form:
$$H_0 : \qquad \beta = 0 $$
vs.
$$H_1 : \qquat \beta \neq 0 $$
Note that linear regression is essentially just an extension of the ANOVA that is that it further allows numerical variable to be part of the testing process. 

Regression in R is done using the `lm` function:
```{r,eval=FALSE}
lm(y~x, data=dat)
```


Now as an example consider the length weight relationship:
```{r}
ggplot(d,aes(length,wgt,col=sex)) + geom_point()
```

we see that the relationship between length and weight is probably not linear but as first approximation try that:
```{r}
fit <- lm(wgt~length,data=d)
fit
```
hmm fish a 0 cm is -2.4 kg. That is not really plausible. But look at the usual summaries:
```{r}
summary(fit)
```

But let's do something more realistic and log transform the data:
```{r}
fit <- lm(log10(wgt)~log10(length),data=d)
fit
```

This looks more sensible. Now lets plot the results:
```{r}
fit %>% 
  augment() %>%
  mutate(length = 10^log10.length.,
         wgt = 10^.fitted) %>% 
  ggplot(aes(length,wgt)) + geom_point(data=d) + geom_line()
```

One can the add variable to the regression by adding to the formula:
```{r}
fit2 <- lm(log10(wgt)~log10(length)+sex,data=d)
summary(fit2)
```

#### Exercise

* Check if there is a significant interaction effect between sex and length in the weight


## Model fitting and diagnostics
There is a whole host of different tools that are useful when building regression type models. If we want perform a stepwise model selection we can use the `step` function 
```{r}
step(fit2)
```

`step` iteratively applies `drop1` to the model until the AIC cannot be improved. Lets try a bigger model:

```{r}
fit.full <- lm(log10(wgt)~log10(length)*sex + country + depth + 
                 as.factor(year),
               d)
summary(fit.full)
```

Now do `drop1`:
```{r}
drop1(fit.full)
```
where we see that veidarfaeri is does not improve the fit. Let's select the minimal model:
```{r}
fit.reduced <- step(fit.full)
```

Next we can plot some diagnostics:
```{r}
plot(fit.reduced)
```


# Non-linear regression
Now say we want to fit a growth curve to our minke whale data
* Typically this would by a Von Bertalanffy growth curve of the form:
$$ l = L_{\infty}(1-e^{-k(a-t_0)})$$
 How do we do this in R?

```{r,echo=FALSE,message=FALSE,warning=FALSE}
ggplot(minke,aes(age,length)) + geom_point() + theme_bw() + ylab('Length') + xlab('Age')
```

What do we want to do exactly?

* Again we want to find the best fitting curve through the datapoints, although now we want estimate a more arbitrary function
* This means that we want to "draw" a line that minimized on average the distance to all data points, i.e. find x that solves
$$min_{x} \left(\sum_{i} (l_i - VonB(\textbf{x},a(i)))^2\right)$$

* In the Von B function there are three parameters, $L_\infty$, $k$ and $t_0$ that can be adjusted so the task here is to find values of these three parameters such that the above sum is minimized


```{r}
age.data <- filter(minke,!is.na(age))
minke.vonB.par <- 
  nls(length~Linf*(1-exp(-K*(age-t0))),
      data=age.data, start=list(Linf=1100, K=0.1, t0=-1))
minke.vonB.par

```


* Formulas in R typically look for variables in the data, in this case the minke whale dataset.
* If a variable is not in the data, such as variables "Linf", "K" and "t0", they are assumed to be parameters that need to be estimated
* Starting values are given in the input as "start". If not given the function may converge to a wrong minima or not at all.

* Confidence intervals
Recall that a 95\% confidence interval represents the potential range of the data, i.e. one can not reject the hypothesis that the parameter estimate is within the range. Confidence intervals can be computed using the following command:
```{r}
minke.vonB.par %>% 
  tidy(conf.int = TRUE)

```

Now lets plot the output:

```{r}
minke.vonB.par %>% 
  augment() %>% 
  ggplot(aes(age,length)) + geom_point() + geom_line(aes(y=.fitted))
```


# Inferences related to categorical variables

## Equality of proportions in a population

Say we have a group $G$, and the probability of being in this group is $p$ (and conversely not being in group $G$ has a probability of $q=1-p$) and we want to investigate the properties of $p$. Then we want to do a binomial test. Essentially this is a hypothesis test of the form
$$H_0:\qquad p = \mu$$ vs one the following $$H_1 : \qquad pÂ¸\neq \mu \quad\vee \quad p < \mu \quad \vee \quad p>\mu $$
In R this is done using `binom.test`:
```{r,eval=FALSE}
binom.test(x = Num.success,   ## number of successes (entries in group G)
           n = Num.trials,    ## total number of trials (size of population)
           p = mu,            ## the hypothesised propability
           alternative = 'two.sided')
```


To illustrate this take the sex ratio from the Ellidaar data and ask the question, is the ratio even? 

```{r}
## restrict the data to only male and females
d2 <- 
  d %>% 
  filter(sex %in% c('M','F'))
```

Now test the hypothesis that the sex ratio is even:

```{r}
kyn.tab <- table(d2$sex)

binom.test(kyn.tab)
```

We see immediately that the sex ratio is not even in the dataset, i.e. the null hypothesis is rejected. In addition we see that the `binom.test` function calculates an estimate for the ratio and a 95% confidence interval. These values can be saved into a data.frame using the `tidy` function from the `broom` package:

```{r}
binom.test(kyn.tab) %>% 
  tidy()
```

`binom.test` allows for slight variations in t


## Comparing two or more population proportions

Expanding the binomial test above, say if we wanted to look at whether the sex ratio differs by year, wwe will need to use a $\chi^2$-test. Essentially the $\chi^2$-test compares the propability of being in group $G$ by category $i$, i.e. tests a hypothesis of the form:

$$H_0 : \qquad p_1=p_2=\ldots=p_n$$

vs. 

$$H_1 : \qquad \exists i,j\quad s.t\quad p_i\neq p_j$$

The `chisq.test` has a range of options:
```{r,eval=FALSE}
chisq.test(Cat1,                   ## Factor indicating group assignment
           Cat2,                   ## -- || --
           p,                      ## if y is null, these are the proportions in 
                                   ## each of factor levels
           simulate.p.value = FALSE) ## should the p-value be simulated using MC methods
```

Taking this to our river data, we want to test if the sex ratio is different between years:
```{r}
chisq.test(d2$sex,d2$year)
```

and again we can use `tidy` to get the estimates and test statistics from the test:
```{r}
chisq.test(d2$sex,d2$year) %>% 
  tidy()
```

### Excercise

* Is the proportion of fish caught using a fly significantly different from other gear types? 
* Are the number of releases significantly different between the sexes?


# Inferences related to numerical variables

## Comparing means

When comparing means there are a number of situations that one might be interested in:

* Is the population mean equal to a certain number? 
* Are means of two populations equal?
* Is there an increase between measurements from the same subject?

This is all done using a $t$-test:
```{r,eval=FALSE}
t.test(x,                         ## measurements from population 1
       y,                         ## measurements from population 2
       mu,                        ## true difference between populations
       paired = FALSE,            ## is this a paired test
       alternative = "two.sided") ## the alternative hypothesis
```

Note that by default `t.test` corrects for difference is variance between populations, this can be turned off by setting `var.equal` to `TRUE` when calling `t.test`.

### One population

Looking at the mean length in the river data:

```{r}
t.test(d$length)
```

we see that the mean length is significantly different from 0. Notice that the `t.test` gives the confidence interval for the mean length. 

And as usual we can get the output using `tidy`
```{r}
t.test(d$length) %>% 
  tidy()
```

### Two populations

Lets test if the mean length is significantly different between males and females:
```{r}
males <- d %>% filter(sex == 'M')
females <- d %>% filter(sex == 'F')

t.test(males$length,females$length)
```

and we can set the alternative hypothesis such that the males are smaller than females:
```{r}
t.test(males$length,females$length,alternative = 'less')
```

Then we cannot reject the hypothesis that the true difference is 0. 

## Comparing variance of two (or more) populations

When we want to test wheather the variances of two (or more) populations are equal, i.e. test a hypothesis of the form:

$$H_0 :\qquad \sigma_1^2=\sigma_2^2=\ldots=\sigma_n^2$$
vs.
$$ H_1 : \qquad \exists i,j\quad s.t.\quad \sigma_i^2\neq\sigma_j^2 $$
one can use Bartlett's test of homogeneitiy of variances:
```{r,eval=FALSE}
bartlett.test(x,  ## measurements by group
              g)  ## factor indicating group
```

As an example we can ask if the variances in length are constant between the sexes:
```{r}
bartlett.test(d2$length,
              d2$sex)
```
### Exercise
* Do males and females differ significantly in weight?
* What is the confidence interval for the mean length of males?


# Analysis of variance

Analysis of variance (or ANOVA) is essentially a test for the equality of means between two (or more) gropus. In general the hypothesis we want to test are of the form:
$$H_0 :\qquad \mu_1=\mu_2=\ldots=\mu_n$$
vs.
$$ H_1 : \qquad \exists i,j\quad s.t.\quad \mu_i\neq\mu_j $$
In R one uses the `aov` function to test this class of hypotheses:
```{r,eval=FALSE}
aov(y~x, data = dat)
```
This need a bit of explaining. The first part of the `aov` input is a formula object. A formula is generally of the form:
$$ y \tilde \quad x$$
where $y$ is the reponse variable and $x$ is the predictor variable. Formulas call on columns from the data and can be composed of number of components:

```{r,eval=FALSE}
+       ## add variables to the formula
-       ## remove variable
:       ## interaction, written x:y
*       ## x*y is a shorthand for x + y + x:y
/       ## x/y is a shorthand for x + x:y
|       ## x|y is x conditioned on the values of y
I()     ## include a new variable based on the calculations within the parenthesis
-1      ## no intercept term
```


## One-way anova
As illustration of how one would perform an ANOVA in R consider the mean length caught by year:

```{r}
ggplot(d,aes(year,length,group=round(year))) + geom_boxplot()
```



```{r}
## note we need change the year to factor 
fit <- aov(length~as.factor(year),data=d)
fit
```
To get the results from the ANOVA one typically needs to use the `summary` function:
```{r}
summary(fit)
```
where we see the mean length is significantly different by year. And as above we can use `tidy`
```{r}
fit %>% tidy()
```

Now this is all well and good, but we now need to know which of these years are significantly different. This can be done using Tukey test, implemented using the `lsmeans` function:
```{r}
fit %>% lsmeans(pairwise~year)
```
But getting the results to a data.frame is bit more involved:
```{r}
ls.fit <- fit %>% lsmeans(pairwise~year)
ls.cont <- ls.fit$contrasts %>% summary() %>% as_data_frame()
ls.cont
```
And to find the years:
```{r}
ls.cont %>% filter(p.value < 0.05)
```

## Two way anova example
As an example of how to do a two way anova with interactions, consider length by sex and year: 
```{r}
ggplot(d,aes(year,length, group=interaction(round(year),sex),fill = sex)) +
  geom_boxplot()
```

```{r}
two.way <- aov(length~sex*as.factor(year),data=d)
```
and we can test significance using `summary`:
```{r}
summary(two.way)
```
or using variable deletion using `drop1`:
```{r}
drop1(two.way,test='F')
```
here we see that the interaction is significant. 

### Excercise 

* Test if the weight is significantly different by year



